---
output: html_document
---

# Machine Learning Aissgnment: Exercise Quality Prediction from Sensor Data

### Marvin J. Rich
### February 12th, 2017

## Document Overview

This report evaluates sensor data, placed on various subjects and exercise equipment, to determine if the level of correct exercise format quality can be predicted utilizing just the data from these sensor observations. Training data was used to tune multiple machine learning method models. Finally a one time test set was used to predict an exercise quality level (A-E) based solely on observation data.

## Exploratory Analysis and Data Reduction

This code reads in the testing and training data into R:

```{r echo=TRUE,cache=TRUE,message=FALSE,warning=FALSE}
#-- access libraries
library(caret)
library(ggplot2)

#-- download testing & training datasets 
download.file(url="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
                  destfile="training.csv")
download.file(url="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
                  destfile="testing.csv")

#-- Read training and testing data into R
training <- read.csv("training.csv")
testing  <- read.csv("testing.csv")
numDataVars <- dim(training)[2]

```

There are `r numDataVars` variables in the training and test data. This large number is detrimental to machine learning performance. 

In order to reduce the number of variables used in model training and prediction, the philosophy was used to just capture the measurements that were associated with movement, since this data is closest to influencing exercise correctness decisions. Here is the R code that reduced input variables based on the previous philosophy:


```{r echo=TRUE,cache=TRUE}
#-- Set seed for reproducability
set.seed(1234)
#-- Get dimention of the data
num_vars <- dim(training)
#-- remove 1st seven irrelevent variables
t1 <- training[,-(1:7)]
#-- remove factor variables
notFactors <- sapply(1:length(names(t1)), function(x) class(t1[,x])!="factor")
t2 <- t1[,notFactors]
#-- remove variables with > 90% NA's
notNA <- sapply(1:length(names(t2)), function(x) !(sum(is.na(t2[,x]))/length(t2[,x]) > 0.9))
t3 <- t2[,notNA]
#-- use final training vars in final testing set (append problem_id)
testing <- cbind(testing[,names(t3)],problem_id=testing$problem_id)
#-- append the outcome variable in final training set
training <- cbind(t3,classe=training$classe)
names(training)
numVars <- length(names(training))
```

The number of variables was reduced from `r numDataVars` to `r numVars` by 1) eliminating variables not relevant to the problem (i.e. names, dates, etc.), 2) eliminating factor variables not relevant to instrument observation data, and 3) eliminating variables with a preponderance of missing data (NA's). The outcome variable, classe, must be included (even though it is also a factor variable, it is what we'll be predicting):

## Partitioning of Training Data

Once we've arrived at the number of variables to use, the original training data is partitioned into separate training and test sets. Since this data is our "in sample" data for internal training and testing. The original testing data was only used for a one time final prediction. 

```{r echo=TRUE,cache=TRUE}
#-- Data Partitioning
inTrain <- createDataPartition(y=training$classe,p=0.7,list=FALSE)
trn <- training[inTrain,]
tst <- training[-inTrain,]
```

## Cross Validataion
In order to assure the testing data yields a reasonable accuracy with respect to new data, K-Fold cross validation was used on all models. It utilizes three re-samples of the testing data to get an average of the prediction accuracy, which will be more reflective of the actual testing accuracy one would experience with new data.

```{r echo=TRUE,cache=TRUE}
#-- K-Fold Cross Validation Used
crossVal <- trainControl(method="cv",number=3)
```

## Model Selection, Fitting, Prediction and Accuracy 

A total of three candidate machine learning algorithm models were selected for evaluation: a boosting model (gbm), a random forest model (rf), and a naive Bayes model (nb).

The three selected prediction models were fitted using the training subset of the original training data. The test (in sample) prediction was then performed utilizing the remaining test portion of the original training data.

```{r echo=TRUE,cache=TRUE,message=FALSE,warning=FALSE}
#-- Fit a gbm, rf, and nb model with the in-sample train data
gbmMdl <- train(classe ~ ., data=trn,trControl=crossVal,
                            method="gbm",verbose=FALSE) 
rfMdl <- train(classe ~ ., data=trn,trControl=crossVal,
                            method="rf") 
nbMdl <- train(classe ~ ., data=trn,trControl=crossVal,
                            method="nb") 
#-- Perform prediction on each model with the in-sample test data
gbmPred <- predict(gbmMdl,tst) 
rfPred  <- predict(rfMdl,tst)  
nbPred  <- predict(nbMdl,tst)  
```

The prediction performance of each algorithm was evaluated based on the prediction outcome of the testing portion of the original testing data. We can see from the following plot that, in terms of prediction accuracy, the RF model is 1st, followed by the gbm model, and finally the nb model.

```{r echo=TRUE,cache=TRUE}
#-- Get Performance Metrics for each model
modPerf <- resamples(list(gbm=gbmMdl, rf=rfMdl, nb=nbMdl))
#-- Plot the performance metrics of the models
mdlplot <-bwplot(modPerf,layout=c(2,1))
mdlplot
```

We utilized confusionMatrix() function output data to get actual accuracy values:

```{r echo=TRUE,cache=TRUE}
#-- Get accuracy values via confusion matricies
gbmCM <- confusionMatrix(gbmPred,tst$classe)
rfCM  <- confusionMatrix(rfPred,tst$classe)
nbCM  <- confusionMatrix(nbPred,tst$classe)
accSummary <- data.frame(ModelType=c("gbm","rf","nb"),
                         Accuracy=rbind(gbmCM$overall[1],
                                        rfCM$overall[1],
                                        nbCM$overall[1]))  
print(accSummary)
bestModel <- rfMdl
bestAccuracy <- round(rfCM$overall[1]*100,2)
```

We see that the rf (random forest) model has the best accuracy of `r bestAccuracy` percent. With this high accuracy percentage, it doesn't appear that additional processing, such as model stacking, was needed to boost the overall accuracy further. So the random forest (rf) model was used for final prediction.

The following is the confusion matrix statistics for the selected random forest model:

```{r echo=TRUE,cache=TRUE}
rfCM
```

From the confusion matrix in the output, we can see that the actual versus predicted classe variable is correct most of the time which illustrates the `r bestAccuracy`% accuracy rate of the random forest model. The sample error
comes out to be `r 100-bestAccuracy` %. With the cross validation as a basis of model fitting, it is concluded that accuracy (and sample error) will be in this range also for new predictions.


## Final Prediction on Testing Data

The random forest (rf) model was used to perform the final prediction on the original testing data. The testing data has 20 observations from which an each exercise quality class (A-E) was predicted for each observation:

```{r echo=TRUE,cache=TRUE}
finalPred <- predict(bestModel,testing)
print(finalPred)
```

## Conclusions

The previous analysis of exercise sensory data utilized data exploration to assess the data and reduce the number of prediction variables. A total of three models were introduced to assess their predictive capabilities. A boosting model, a random forrest model, and a naive bayes model. Cross validation was used on the selected models to further enhance their predictive capabilities on training data. Of the three candidate machine learning models, the random forests model was chosen based on its prediction accuracy (`r bestAccuracy`%) on the test data. Finally the random forests model was used to predict the exercise quality classification for twenty independent observations. 